{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Objectives\" data-toc-modified-id=\"Objectives-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Objectives</a></span></li><li><span><a href=\"#Decision-Trees-at-a-High-Level\" data-toc-modified-id=\"Decision-Trees-at-a-High-Level-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Decision Trees at a High Level</a></span><ul class=\"toc-item\"><li><span><a href=\"#Simple-Example-of-a-Decision-Tree\" data-toc-modified-id=\"Simple-Example-of-a-Decision-Tree-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Simple Example of a Decision Tree</a></span><ul class=\"toc-item\"><li><span><a href=\"#Picturing-Decisions-as-a-Tree\" data-toc-modified-id=\"Picturing-Decisions-as-a-Tree-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Picturing Decisions as a Tree</a></span></li></ul></li><li><span><a href=\"#Overview-of-Algorithm's-Steps\" data-toc-modified-id=\"Overview-of-Algorithm's-Steps-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Overview of Algorithm's Steps</a></span></li></ul></li><li><span><a href=\"#Entropy/Information-Gain-and-Gini\" data-toc-modified-id=\"Entropy/Information-Gain-and-Gini-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Entropy/Information Gain and Gini</a></span><ul class=\"toc-item\"><li><span><a href=\"#Entropy-of-a-Split\" data-toc-modified-id=\"Entropy-of-a-Split-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Entropy of a Split</a></span></li><li><span><a href=\"#Gini-Impurity\" data-toc-modified-id=\"Gini-Impurity-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Gini Impurity</a></span></li></ul></li><li><span><a href=\"#Regression\" data-toc-modified-id=\"Regression-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Regression</a></span></li><li><span><a href=\"#With-sklearn\" data-toc-modified-id=\"With-sklearn-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>With <code>sklearn</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#Important-Terminology-related-to-Decision-Trees\" data-toc-modified-id=\"Important-Terminology-related-to-Decision-Trees-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Important Terminology related to Decision Trees</a></span></li></ul></li><li><span><a href=\"#Challenges-with-Decision-Trees\" data-toc-modified-id=\"Challenges-with-Decision-Trees-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Challenges with Decision Trees</a></span><ul class=\"toc-item\"><li><span><a href=\"#Decision-trees-are-prone-to-overfitting\" data-toc-modified-id=\"Decision-trees-are-prone-to-overfitting-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Decision trees are prone to overfitting</a></span></li><li><span><a href=\"#Bias-Variance-with-Decision-Trees\" data-toc-modified-id=\"Bias-Variance-with-Decision-Trees-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Bias-Variance with Decision Trees</a></span></li><li><span><a href=\"#Stopping-Criterion---Pruning-Parameters\" data-toc-modified-id=\"Stopping-Criterion---Pruning-Parameters-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Stopping Criterion - Pruning Parameters</a></span></li></ul></li><li><span><a href=\"#Feature-Importances\" data-toc-modified-id=\"Feature-Importances-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Feature Importances</a></span></li><li><span><a href=\"#Conclusions\" data-toc-modified-id=\"Conclusions-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Conclusions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pros\" data-toc-modified-id=\"Pros-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Pros</a></span></li><li><span><a href=\"#Cons\" data-toc-modified-id=\"Cons-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Cons</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T03:56:22.375115Z",
     "start_time": "2021-05-05T03:56:20.462756Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, plot_tree\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Describe the decision tree modeling algorithm\n",
    "- Use attribute selection methods to build different trees\n",
    "- Explain the pros and cons of decision trees\n",
    "- Interpret the feature importances of a fitted model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Decision Trees at a High Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> **Decision trees** are a supervised learning model that makes uses past data to form a graph/pathway which leads to the model making _decisions_ on it's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "I like to think of decision trees to be a bunch of forks in the road."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<a title=\"Jonathan Billinger / Fork in the road\" href=\"https://commons.wikimedia.org/wiki/File:Fork_in_the_road_-_geograph.org.uk_-_1355424.jpg\"><img width=\"512\" alt=\"Fork in the road - geograph.org.uk - 1355424\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/71/Fork_in_the_road_-_geograph.org.uk_-_1355424.jpg\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Every time we make a decision, we split up, or _partition_, the data based on the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Simple Example of a Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's say we have this set of data:\n",
    "\n",
    "Work Status |  Age  | Favorite Website\n",
    "------------|-------|-------------------------\n",
    " Student    | Young | A\n",
    " Working    | Young | B\n",
    " Working    | Old   | C\n",
    " Working    | Young | B\n",
    " Student    | Young | A\n",
    " Student    | Young | A\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This can help us answer a couple questions:\n",
    "\n",
    "- If someone is a young worker, what website do we recommend?\n",
    "- If someone is an old worker, what website then?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Picturing Decisions as a Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](img/simple_decision_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note our tree would look different depending on where we made our decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Overview of Algorithm's Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Here's a great visual of a decision tree  http://www.r2d3.us/visual-intro-to-machine-learning-part-1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. Organize data features and target\n",
    "2. Make a *decision* (a split) based on some *metric* using the features\n",
    "    * Data are split into partitions via *branches*\n",
    "3. Continue on with each partition, and do more splits for each using the features in that partition\n",
    "4. Keep doing that until a **stopping condition** is hit\n",
    "    - Number of data points in a final partition\n",
    "    - Layers deep\n",
    "5. To make predictions, run through the decision nodes (the forks in the road)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we have to determine what metric we use to make our split/decision!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Entropy/Information Gain and Gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The goal is to have our ultimate classes be fully \"ordered\" (for a binary dependent variable, we'd have the 1's in one group and the 0's in the other). So one way to assess the value of a split is to measure how *disordered* our groups are, and there is a notion of *entropy* that measures precisely this.\n",
    "\n",
    "The entropy of the whole dataset is given by:\n",
    "\n",
    "$\\large E = -\\Sigma^n_i p_i\\log_2(p_i)$,\n",
    "\n",
    "where $p_i$ is the probability of belonging to the $i$th group, where $n$ is the number of groups (i.e. target values).\n",
    "\n",
    "**Entropy will always be between 0 and 1. The closer to 1, the more disordered your group.**\n",
    "\n",
    "<img src='./img/Entropy_mapped.png' width=600/>\n",
    "\n",
    "In the present case we have only two groups of interest: criminal and civil.\n",
    "\n",
    "Four out of six were civil and two out of six were criminal, so **these are the relevant probabilities** for our calculation of entropy.\n",
    "\n",
    "So our entropy for the sample above is:\n",
    "\n",
    "$-\\frac{2}{3}*\\log_2\\left(\\frac{2}{3}\\right) - \\frac{1}{3}*\\log_2\\left(\\frac{1}{3}\\right)$.\n",
    "\n",
    "Let's use ``numpy's`` `log2()` function to calculate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "-(2/3) * np.log2(2/3) - (1/3) * np.log2(1/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "That's a high level of disorder!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Entropy of a Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To calculate the entropy of a *split*, we're going to want to calculate the entropy of each of the groups made by the split, and then calculate a weighted average of those groups' entropies––weighted, that is, by the size of the groups. Let's calculate the entropy of the split produced by our \"was the case filed before October 2018?\" question:\n",
    "\n",
    "Group 1 (filed before): civil, criminal, civil, civil\n",
    "\n",
    "$E_{g1} = -\\frac{3}{4} * \\log_2(\\frac{3}{4}) - \\frac{1}{4} * \\log_2(\\frac{1}{4})$. \n",
    "\n",
    "Group 2 (filed after): criminal, civil\n",
    "\n",
    "$E_{g2} = -\\frac{1}{2} * \\log_2\\left(\\frac{1}{2}\\right) - \\frac{1}{2} * \\log_2\\left(\\frac{1}{2}\\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ent_before = -(3/4)*np.log2(3/4) - (1/4)*np.log2(1/4)\n",
    "print(ent_before)\n",
    "\n",
    "ent_after = -(1/2)*np.log2(1/2) - (1/2)*np.log2(1/2)\n",
    "print(ent_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now weight those by the probability of each group, and sum them, to find the entropy of the split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "before_norm = (4/6) * ent_before\n",
    "after_norm = (2/6) * ent_after\n",
    "\n",
    "E_split_d = before_norm + after_norm\n",
    "E_split_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Not great. Compare that to the Mueller question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# In Group 1: civil, criminal, criminal\n",
    "\n",
    "ent_mueller = -(2/3)*np.log2(2/3) - (1/3)*np.log2(1/3)\n",
    "print(ent_mueller)\n",
    "\n",
    "# In Group 2: civil, civil, civil\n",
    "\n",
    "ent_other = -(3/3)*np.log2(3/3) # Pure group!\n",
    "print(ent_other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Weighted sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mueller_norm = ent_mueller * 3/6\n",
    "other_norm = ent_other * 3/6\n",
    "\n",
    "E_split_m = mueller_norm + other_norm\n",
    "E_split_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For a given split, the **information gain** is simply the entropy of the parent group less the entropy of the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "total_entropy_sample = -(4/6)*np.log2(4/6) - (2/6)*np.log2(2/6)\n",
    "\n",
    "\n",
    "# Information gain: before or after October 2018\n",
    "\n",
    "ig_d = total_entropy_sample - E_split_d\n",
    "print(f\"Information gain for Oct. 2018 split: {ig_d}\")\n",
    "\n",
    "# Information gain: Mueller-related or not\n",
    "\n",
    "ig_m = total_entropy_sample - E_split_m\n",
    "print(f\"Information gain for Mueller split: {ig_m}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For a given parent, then, we maximize our model's performance by *minimizing* the split's entropy.\n",
    "\n",
    "What we'd like to do then is:\n",
    "\n",
    "1. to look at the entropies of all possible splits, and\n",
    "2. to choose the split with the lowest entropy.\n",
    "\n",
    "In practice there are far too many splits for it to be practical for a person to calculate all these different entropies ...\n",
    "\n",
    "... but we can make computers do these calculations for us!\n",
    "\n",
    "Moreover, we can **iterate** this algorithm on the resultant groups until we reach pure groups!\n",
    "\n",
    "**Question**: Are we in fact guaranteed, proceeding in this way, to reach pure groups, no matter what our data looks like?\n",
    "\n",
    "**Observation**: This algorithm looks for the best split **locally**. There is no regard for how an overall tree might look. That's what makes this algorithm ***greedy***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Gini Impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "An alternative metric to entropy comes from the work of Corrado Gini. The Gini Impurity is defined as:\n",
    "\n",
    "$\\large G = 1 - \\Sigma_ip_i^2$, or, equivalently, $\\large G = \\Sigma_ip_i(1-p_i)$.\n",
    "\n",
    "where, again, $p_i$ is the probability of belonging to the $i$th group.\n",
    "\n",
    "**Gini Impurity will always be between 0 and 0.5. The closer to 0.5, the more disordered your group.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# With `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# This will turn dates into numbers!\n",
    "\n",
    "df['dateFiled'] = pd.to_datetime(df['dateFiled']).map(lambda x: x.date().toordinal())\n",
    "\n",
    "# And this will convert the issue to numbers:\n",
    "\n",
    "df['issue'] = df['issue'].map(lambda x: 1 if x == 'Mueller investigation' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "X = df.drop('type', axis=1)\n",
    "y = df.type\n",
    "dtree = dt.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_tree(dt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# April 7, 2018 is halfway between the points that differ in type\n",
    "\n",
    "datetime.fromordinal(736791)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Important Terminology related to Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- **Root Node:** Represents entire population or sample.\n",
    "- **Decision Node:** Node that is split.\n",
    "- **Leaf/ Terminal Node:** Node with no children.\n",
    "- **Pruning:** Removing nodes.\n",
    "- **Branch / Sub-Tree:** A sub-section of a decision tree.\n",
    "- **Parent and Child Node:** A node divided into sub-nodes is the parent; the sub-nodes are its children.\n",
    "\n",
    "<img src='./img/decision_leaf.webp' width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Challenges with Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Decision trees are prone to overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('data/trump-lawsuits.csv',\n",
    "                     usecols=['dateFiled', 'type', 'issue']).iloc[[5, 17, 20, 25], :]\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Accuracy on training data\n",
    "dt.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Dates to numbers\n",
    "\n",
    "df_test['dateFiled'] = pd.to_datetime(df_test['dateFiled'])\\\n",
    ".map(lambda x: x.date().toordinal())\n",
    "\n",
    "# And this will convert the issue to numbers:\n",
    "\n",
    "df_test['issue'] = df_test['issue']\\\n",
    ".map(lambda x: 1 if x == 'Mueller investigation' else 0)\n",
    "\n",
    "X_test = df_test.drop('type', axis=1)\n",
    "y_test = df_test.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Accuracy on test data\n",
    "dt.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Bias-Variance with Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The CART algorithm will repeatedly partition data into smaller and smaller subsets until those final subsets are homogeneous in terms of the outcome variable. In practice this often means that the final subsets (known as the leaves of the tree) each consist of only one or a few data points. \n",
    "\n",
    "This tends to result in low-bias, high variance models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Stopping Criterion - Pruning Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The recursive binary splitting procedure described above needs to know when to stop splitting as it works its way down the tree with the training data.\n",
    "\n",
    "**min_samples_leaf:**  The most common stopping procedure is to use a minimum count on the number of training instances assigned to each leaf node. If the count is less than some minimum then the split is not accepted and the node is taken as a final leaf node.\n",
    "\n",
    "**max_leaf_nodes:** \n",
    "Reduce the number of leaf nodes.\n",
    "\n",
    "**max_depth:**\n",
    "Reduce the depth of the tree to build a generalized tree.\n",
    "\n",
    "**min_impurity_split :**\n",
    "A node will split if its impurity is above the threshold, otherwise it will be a leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Feature Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The fitted tree has an attribute called `ct.feature_importances_`. What does this mean? Roughly, the importance (or \"Gini importance\") of a feature is a sort of weighted average of the impurity decrease at internal nodes that make use of the feature. The weighting comes from the number of samples that depend on the relevant nodes.\n",
    "\n",
    "> The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance. See [`sklearn`'s documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.feature_importances_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "dt.fit(X, y)\n",
    "\n",
    "for fi, feature in zip(dt.feature_importances_, X.columns):\n",
    "    print(fi, feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "More on feature importances [here](https://towardsdatascience.com/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- The decision tree is a \"white-box\" type of ML algorithm. It shares internal decision-making logic, which is not available in the black-box type of algorithms such as Neural Network.\n",
    "- Its training time is faster compared to other algorithms such as neural networks.\n",
    "- The decision tree is a non-parametric method, which does not depend upon probability distribution assumptions.\n",
    "- Decision trees can handle high-dimensional data with good accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Pros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Decision trees:\n",
    "- are easy to interpret and to visualize;\n",
    "- can easily capture non-linear patterns;\n",
    "- require little data preprocessing from the user. For example, there is no need to normalize columns;\n",
    "- can be used for feature engineering such as predicting missing values, suitable for variable selection;\n",
    "- make no assumptions about distribution, because of the non-parametric nature of the algorithm;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Decision trees:\n",
    "- are sensitive to noisy data. This problem can be significantly ameliorated by ensemble methods.\n",
    "- produce high-bised models with imbalanced datasets (so it is recommended that you balance out the dataset before creating the decision tree)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "280.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
